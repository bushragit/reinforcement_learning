{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30839,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Q-Learning",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bushragit/reinforcement_learning/blob/main/Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Qlearning\n",
        "it is model free.  \n",
        "it is off policy means learn from hit and trial  \n",
        "use bellman equation"
      ],
      "metadata": {
        "id": "MvNV_YMdb4EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import gym\n",
        "import random"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:38.248528Z",
          "iopub.execute_input": "2025-01-17T11:31:38.249077Z",
          "iopub.status.idle": "2025-01-17T11:31:38.903104Z",
          "shell.execute_reply.started": "2025-01-17T11:31:38.249029Z",
          "shell.execute_reply": "2025-01-17T11:31:38.901941Z"
        },
        "id": "XK4zfKKZb4Ec"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v1\",map_name=\"4x4\", is_slippery=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:38.904547Z",
          "iopub.execute_input": "2025-01-17T11:31:38.904855Z",
          "iopub.status.idle": "2025-01-17T11:31:38.939411Z",
          "shell.execute_reply.started": "2025-01-17T11:31:38.904829Z",
          "shell.execute_reply": "2025-01-17T11:31:38.937873Z"
        },
        "id": "wnDQTXDpb4Ej"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:38.941409Z",
          "iopub.execute_input": "2025-01-17T11:31:38.941714Z",
          "iopub.status.idle": "2025-01-17T11:31:38.949963Z",
          "shell.execute_reply.started": "2025-01-17T11:31:38.941688Z",
          "shell.execute_reply": "2025-01-17T11:31:38.948895Z"
        },
        "id": "3Ob1HJihb4Eo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#possible actions in this case are 4\n",
        "print(\"\\n---Action Space____\\n\")\n",
        "print(\"Action space shape:\",env.action_space.n)\n",
        "print('Action space sample:',env.action_space.sample()) #take a random action)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:38.951772Z",
          "iopub.execute_input": "2025-01-17T11:31:38.952215Z",
          "iopub.status.idle": "2025-01-17T11:31:38.969287Z",
          "shell.execute_reply.started": "2025-01-17T11:31:38.952168Z",
          "shell.execute_reply": "2025-01-17T11:31:38.968177Z"
        },
        "id": "OiyIDpsEb4Es"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The action space (the set of possible actions the agent can take) is discrete with 4 actions available ðŸŽ®:\n",
        "\n",
        "0. GO LEFT  \n",
        "1. GO DOWN  \n",
        "2. GO RIGHT\n",
        "3. GO UP"
      ],
      "metadata": {
        "id": "Fv4qEu2ub4Eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create and initialize the Q table"
      ],
      "metadata": {
        "id": "m3adlgrOb4Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_space = env.observation_space.n\n",
        "action_space = env.action_space.n\n",
        "print(\"There are \", state_space, \" possible states\") #locations where it can move\n",
        "print(\"There are \", action_space, \" possible actions\")#how it move above,below,left,right"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:38.970478Z",
          "iopub.execute_input": "2025-01-17T11:31:38.970893Z",
          "iopub.status.idle": "2025-01-17T11:31:38.992041Z",
          "shell.execute_reply.started": "2025-01-17T11:31:38.970828Z",
          "shell.execute_reply": "2025-01-17T11:31:38.990751Z"
        },
        "id": "rdtY2fdCb4E5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#states in no of rows and actions in no of columns\n",
        "def initialize_Q_table(state_space,action_space):\n",
        "    Qtable=np.zeros((state_space,action_space))\n",
        "    return Qtable"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:38.993111Z",
          "iopub.execute_input": "2025-01-17T11:31:38.993529Z",
          "iopub.status.idle": "2025-01-17T11:31:39.013042Z",
          "shell.execute_reply.started": "2025-01-17T11:31:38.993488Z",
          "shell.execute_reply": "2025-01-17T11:31:39.01175Z"
        },
        "id": "9pOVZf-vb4E9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_froznlake = initialize_Q_table(state_space,action_space)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:39.014465Z",
          "iopub.execute_input": "2025-01-17T11:31:39.014875Z",
          "iopub.status.idle": "2025-01-17T11:31:39.036083Z",
          "shell.execute_reply.started": "2025-01-17T11:31:39.014834Z",
          "shell.execute_reply": "2025-01-17T11:31:39.034919Z"
        },
        "id": "E2TxUdQOb4E_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_froznlake"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:39.039957Z",
          "iopub.execute_input": "2025-01-17T11:31:39.04032Z",
          "iopub.status.idle": "2025-01-17T11:31:39.059189Z",
          "shell.execute_reply.started": "2025-01-17T11:31:39.040287Z",
          "shell.execute_reply": "2025-01-17T11:31:39.057633Z"
        },
        "id": "XtE2TLisb4FA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_froznlake.shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:39.06153Z",
          "iopub.execute_input": "2025-01-17T11:31:39.062024Z",
          "iopub.status.idle": "2025-01-17T11:31:39.083399Z",
          "shell.execute_reply.started": "2025-01-17T11:31:39.061987Z",
          "shell.execute_reply": "2025-01-17T11:31:39.081998Z"
        },
        "id": "0YhhnxPeb4FF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. epsilon greedy policy (exploitation: analyze on which action has max value)\n",
        "2. With probability É›: we do exploration (trying random action)."
      ],
      "metadata": {
        "id": "PTIaW8mlb4FG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_policy(Qtable,state,epsilon):\n",
        "    random_num=random.uniform(0,1)\n",
        "    if random_num>epsilon:\n",
        "        action = np.argmax(Qtable[state])\n",
        "\n",
        "    else:\n",
        "         action = env.action_space.sample()\n",
        "    return action"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:39.08443Z",
          "iopub.execute_input": "2025-01-17T11:31:39.084706Z",
          "iopub.status.idle": "2025-01-17T11:31:39.103972Z",
          "shell.execute_reply.started": "2025-01-17T11:31:39.084683Z",
          "shell.execute_reply": "2025-01-17T11:31:39.102722Z"
        },
        "id": "cPai9FlMb4FH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_policy(Qtable,state):\n",
        "    action = np.argmax(Qtable[state])\n",
        "    return action"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:39.105213Z",
          "iopub.execute_input": "2025-01-17T11:31:39.105644Z",
          "iopub.status.idle": "2025-01-17T11:31:39.124296Z",
          "shell.execute_reply.started": "2025-01-17T11:31:39.105602Z",
          "shell.execute_reply": "2025-01-17T11:31:39.123137Z"
        },
        "id": "cHG2ySuUb4FI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "n_training_episodes = 10000  # Total training episodes\n",
        "learning_rate = 0.7          # Learning rate\n",
        "\n",
        "# Evaluation parameters\n",
        "n_eval_episodes = 100        # Total number of test episodes\n",
        "\n",
        "# Environment parameters\n",
        "env_id = \"FrozenLake-v1\"     # Name of the environment\n",
        "max_steps = 99               # Max steps per episode\n",
        "gamma = 0.95                 # Discounting rate\n",
        "eval_seed = []               # The evaluation seed of the environment\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.05            # Minimum exploration probability\n",
        "decay_rate = 0.0005            # Exponential decay rate for exploration prob"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:39.125923Z",
          "iopub.execute_input": "2025-01-17T11:31:39.126242Z",
          "iopub.status.idle": "2025-01-17T11:31:39.146157Z",
          "shell.execute_reply.started": "2025-01-17T11:31:39.126212Z",
          "shell.execute_reply": "2025-01-17T11:31:39.145008Z"
        },
        "id": "Do9K3infb4FJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#initially epsilon vale 1 here itis exploration is 1 b/c it's start explore env\n",
        "#inc epsilon inc exploration\n",
        "#dec epsion inc exploitation"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:39.147417Z",
          "iopub.execute_input": "2025-01-17T11:31:39.147742Z",
          "iopub.status.idle": "2025-01-17T11:31:39.168165Z",
          "shell.execute_reply.started": "2025-01-17T11:31:39.147713Z",
          "shell.execute_reply": "2025-01-17T11:31:39.166754Z"
        },
        "id": "m7kpmRh8b4FK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### create the training loop method"
      ],
      "metadata": {
        "id": "D1ToM6R1b4FK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  for episode in range(n_training_episodes):\n",
        "\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) # Reduce epsilon (because we need less and less exploration)\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "\n",
        "    # repeat\n",
        "    for step in range(max_steps):\n",
        "\n",
        "      # Choose the action At using epsilon greedy policy\n",
        "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
        "\n",
        "      # Take action At and observe Rt+1 and St+1\n",
        "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "      new_state, reward, done, info = env.step(action) #Takes the chosen action in the environment\n",
        "\n",
        "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "    #updates the Q-value for the state-action pair using the Q-learning formula\n",
        "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
        "\n",
        "      # If done, finish the episode\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "      # Our state is the new state\n",
        "      state = new_state\n",
        "\n",
        "  return Qtable\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:39.169451Z",
          "iopub.execute_input": "2025-01-17T11:31:39.169919Z",
          "iopub.status.idle": "2025-01-17T11:31:39.196317Z",
          "shell.execute_reply.started": "2025-01-17T11:31:39.169835Z",
          "shell.execute_reply": "2025-01-17T11:31:39.195088Z"
        },
        "id": "G1q6vdPcb4FM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_froznlake)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:39.197581Z",
          "iopub.execute_input": "2025-01-17T11:31:39.197987Z",
          "iopub.status.idle": "2025-01-17T11:31:41.417434Z",
          "shell.execute_reply.started": "2025-01-17T11:31:39.197946Z",
          "shell.execute_reply": "2025-01-17T11:31:41.415814Z"
        },
        "id": "-NzBo6jGb4FO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_frozenlake"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:41.418507Z",
          "iopub.execute_input": "2025-01-17T11:31:41.418928Z",
          "iopub.status.idle": "2025-01-17T11:31:41.425922Z",
          "shell.execute_reply.started": "2025-01-17T11:31:41.418873Z",
          "shell.execute_reply": "2025-01-17T11:31:41.42475Z"
        },
        "id": "aUiPnTdKb4FO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_and_plot_with_std(env, Qtable, n_eval_episodes, max_steps):\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for episode in range(n_eval_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action = np.argmax(Qtable[state])  # Greedy policy\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            state = new_state\n",
        "\n",
        "        rewards_per_episode.append(episode_reward)\n",
        "\n",
        "    # Calculate average reward\n",
        "    average_reward = np.mean(rewards_per_episode)\n",
        "\n",
        "    # Calculate standard deviation of rewards\n",
        "    std_reward = np.std(rewards_per_episode)\n",
        "\n",
        "    return average_reward, std_reward"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:41.427213Z",
          "iopub.execute_input": "2025-01-17T11:31:41.427563Z",
          "iopub.status.idle": "2025-01-17T11:31:41.446926Z",
          "shell.execute_reply.started": "2025-01-17T11:31:41.427532Z",
          "shell.execute_reply": "2025-01-17T11:31:41.445848Z"
        },
        "id": "KWVXbi75b4FP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate our Agent\n",
        "#mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
        "mean_reward, std_reward = evaluate_and_plot_with_std(env, Qtable_frozenlake, n_eval_episodes, max_steps)\n",
        "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:41.448094Z",
          "iopub.execute_input": "2025-01-17T11:31:41.448401Z",
          "iopub.status.idle": "2025-01-17T11:31:41.489845Z",
          "shell.execute_reply.started": "2025-01-17T11:31:41.448345Z",
          "shell.execute_reply": "2025-01-17T11:31:41.488874Z"
        },
        "id": "jYmeAYFNb4FP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8*8 map and slippery"
      ],
      "metadata": {
        "id": "q5lOaLDfb4FR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", is_slippery=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:41.490989Z",
          "iopub.execute_input": "2025-01-17T11:31:41.491429Z",
          "iopub.status.idle": "2025-01-17T11:31:41.498189Z",
          "shell.execute_reply.started": "2025-01-17T11:31:41.491378Z",
          "shell.execute_reply": "2025-01-17T11:31:41.497047Z"
        },
        "id": "EdwCjGWCb4FS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:41.499442Z",
          "iopub.execute_input": "2025-01-17T11:31:41.499863Z",
          "iopub.status.idle": "2025-01-17T11:31:41.519642Z",
          "shell.execute_reply.started": "2025-01-17T11:31:41.499808Z",
          "shell.execute_reply": "2025-01-17T11:31:41.518507Z"
        },
        "id": "6PALkWgXb4FS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "state_space = env.observation_space.n\n",
        "action_space= env.action_space.n\n",
        "print(state_space)\n",
        "print(action_space)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:41.520702Z",
          "iopub.execute_input": "2025-01-17T11:31:41.521155Z",
          "iopub.status.idle": "2025-01-17T11:31:41.542611Z",
          "shell.execute_reply.started": "2025-01-17T11:31:41.521111Z",
          "shell.execute_reply": "2025-01-17T11:31:41.541606Z"
        },
        "id": "AOSjzPJib4FT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_Q_table(state_space,action_space):\n",
        "    Qtable=np.zeros((state_space,action_space))\n",
        "\n",
        "    return Qtable"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:41.543854Z",
          "iopub.execute_input": "2025-01-17T11:31:41.54416Z",
          "iopub.status.idle": "2025-01-17T11:31:41.562598Z",
          "shell.execute_reply.started": "2025-01-17T11:31:41.544133Z",
          "shell.execute_reply": "2025-01-17T11:31:41.561484Z"
        },
        "id": "dc51pqL4b4FU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "frozen_lake = initialize_Q_table(state_space,action_space)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:41.567016Z",
          "iopub.execute_input": "2025-01-17T11:31:41.567322Z",
          "iopub.status.idle": "2025-01-17T11:31:41.581296Z",
          "shell.execute_reply.started": "2025-01-17T11:31:41.567299Z",
          "shell.execute_reply": "2025-01-17T11:31:41.58013Z"
        },
        "id": "mZ1qaBvjb4FU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "frozen_lake.shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:41.583448Z",
          "iopub.execute_input": "2025-01-17T11:31:41.583841Z",
          "iopub.status.idle": "2025-01-17T11:31:41.603663Z",
          "shell.execute_reply.started": "2025-01-17T11:31:41.583813Z",
          "shell.execute_reply": "2025-01-17T11:31:41.602485Z"
        },
        "id": "m3xOI-Web4FV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "frozen_lake"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:41.604657Z",
          "iopub.execute_input": "2025-01-17T11:31:41.604965Z",
          "iopub.status.idle": "2025-01-17T11:31:41.626037Z",
          "shell.execute_reply.started": "2025-01-17T11:31:41.604934Z",
          "shell.execute_reply": "2025-01-17T11:31:41.624714Z"
        },
        "id": "P91QKW6-b4FW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_policy(Qtable,state,epsilon):\n",
        "    random_number=random.uniform(0,1)\n",
        "    if random_number>epsilon:\n",
        "        action=np.argmax(Qtable[state])\n",
        "    else:\n",
        "        action= env.action_space.sample()\n",
        "\n",
        "    return action"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:41.627255Z",
          "iopub.execute_input": "2025-01-17T11:31:41.62759Z",
          "iopub.status.idle": "2025-01-17T11:31:41.647904Z",
          "shell.execute_reply.started": "2025-01-17T11:31:41.627563Z",
          "shell.execute_reply": "2025-01-17T11:31:41.646759Z"
        },
        "id": "2G3woAWHb4FW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_policy(Qtable,state):\n",
        "    action=np.argmax(Qtable[state])\n",
        "    return action"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:41.649035Z",
          "iopub.execute_input": "2025-01-17T11:31:41.649375Z",
          "iopub.status.idle": "2025-01-17T11:31:41.667023Z",
          "shell.execute_reply.started": "2025-01-17T11:31:41.649311Z",
          "shell.execute_reply": "2025-01-17T11:31:41.665773Z"
        },
        "id": "hRGqZqClb4Fe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "n_training_episodes = 10000  # Total training episodes\n",
        "learning_rate = 0.7          # Learning rate\n",
        "\n",
        "# Evaluation parameters\n",
        "n_eval_episodes = 100        # Total number of test episodes\n",
        "\n",
        "# Environment parameters\n",
        "env_id = \"FrozenLake-v1\"     # Name of the environment\n",
        "max_steps = 99               # Max steps per episode\n",
        "gamma = 0.95                 # Discounting rate # This is the discounting rate, which determines the importance of future rewards. A value close to 1 means future rewards are highly considered, while a value close to 0 means the agent prioritizes immediate rewards.\n",
        "eval_seed = []               # The evaluation seed of the environment\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.05            # Minimum exploration probability\n",
        "decay_rate = 0.0005"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:41.668513Z",
          "iopub.execute_input": "2025-01-17T11:31:41.668949Z",
          "iopub.status.idle": "2025-01-17T11:31:41.69094Z",
          "shell.execute_reply.started": "2025-01-17T11:31:41.668903Z",
          "shell.execute_reply": "2025-01-17T11:31:41.6895Z"
        },
        "id": "B6jLODlwb4Ff"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# def train(n_training_episodes, min_epsilon, max_epsilon,env, max_steps, decay_rate, Qtable):\n",
        "#     for episode in range(n_training_episodes):\n",
        "#         epsilon = min_epsilon+(max_epsilon-min_epsilon)*np.exp(-decay_rate*episode)\n",
        "#         state = env.reset()\n",
        "#         step=0\n",
        "#         done=False\n",
        "\n",
        "#         for step in range(max_steps):\n",
        "#             action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
        "#             new_state, reward,info, done = env.step(action)\n",
        "\n",
        "#             Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
        "\n",
        "\n",
        "#             if done:\n",
        "#                 break\n",
        "#             state = new_state\n",
        "\n",
        "#     return Qtable\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:41.692083Z",
          "iopub.execute_input": "2025-01-17T11:31:41.692499Z",
          "iopub.status.idle": "2025-01-17T11:31:41.717341Z",
          "shell.execute_reply.started": "2025-01-17T11:31:41.692461Z",
          "shell.execute_reply": "2025-01-17T11:31:41.715976Z"
        },
        "id": "dTM288lab4Fg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  for episode in range(n_training_episodes):\n",
        "\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) # Reduce epsilon (because we need less and less exploration)\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "\n",
        "    # repeat\n",
        "    for step in range(max_steps):\n",
        "\n",
        "      # Choose the action At using epsilon greedy policy\n",
        "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
        "\n",
        "      # Take action At and observe Rt+1 and St+1\n",
        "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "      new_state, reward, done, info = env.step(action) #Takes the chosen action in the environment\n",
        "\n",
        "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "    #updates the Q-value for the state-action pair using the Q-learning formula\n",
        "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
        "\n",
        "      # If done, finish the episode\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "      # Our state is the new state\n",
        "      state = new_state\n",
        "\n",
        "  return Qtable"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:41.718466Z",
          "iopub.execute_input": "2025-01-17T11:31:41.718869Z",
          "iopub.status.idle": "2025-01-17T11:31:41.741722Z",
          "shell.execute_reply.started": "2025-01-17T11:31:41.718838Z",
          "shell.execute_reply": "2025-01-17T11:31:41.74025Z"
        },
        "id": "srEXIBkNb4Fg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, frozen_lake)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:31:41.742945Z",
          "iopub.execute_input": "2025-01-17T11:31:41.743267Z",
          "iopub.status.idle": "2025-01-17T11:32:00.19385Z",
          "shell.execute_reply.started": "2025-01-17T11:31:41.74324Z",
          "shell.execute_reply": "2025-01-17T11:32:00.192792Z"
        },
        "id": "Jx0ITSuMb4Fm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_frozenlake"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:32:00.194838Z",
          "iopub.execute_input": "2025-01-17T11:32:00.195149Z",
          "iopub.status.idle": "2025-01-17T11:32:00.202678Z",
          "shell.execute_reply.started": "2025-01-17T11:32:00.195122Z",
          "shell.execute_reply": "2025-01-17T11:32:00.201654Z"
        },
        "id": "eeWoels9b4Fn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_agent(env, Qtable, n_eval_episodes, max_steps):\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for episode in range(n_eval_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action = np.argmax(Qtable[state])  # Greedy policy\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            state = new_state\n",
        "\n",
        "        rewards_per_episode.append(episode_reward)\n",
        "\n",
        "    # Calculate average reward\n",
        "    average_reward = np.mean(rewards_per_episode)\n",
        "\n",
        "    # Calculate standard deviation of rewards\n",
        "    std_reward = np.std(rewards_per_episode)\n",
        "\n",
        "    return average_reward, std_reward"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:32:15.496006Z",
          "iopub.execute_input": "2025-01-17T11:32:15.496336Z",
          "iopub.status.idle": "2025-01-17T11:32:15.503172Z",
          "shell.execute_reply.started": "2025-01-17T11:32:15.49631Z",
          "shell.execute_reply": "2025-01-17T11:32:15.501776Z"
        },
        "id": "iRKYxEIMb4Fn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate our Agent\n",
        "mean_reward, std_reward = evaluate_agent(env, Qtable_frozenlake, n_eval_episodes, max_steps)\n",
        "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-17T11:34:36.291176Z",
          "iopub.execute_input": "2025-01-17T11:34:36.291576Z",
          "iopub.status.idle": "2025-01-17T11:34:36.467044Z",
          "shell.execute_reply.started": "2025-01-17T11:34:36.291546Z",
          "shell.execute_reply": "2025-01-17T11:34:36.465685Z"
        },
        "id": "b7w1tQ3Rb4GM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Taxi"
      ],
      "metadata": {
        "id": "eAwo_xNjb4GN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "p7EziEMGb4GP"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}